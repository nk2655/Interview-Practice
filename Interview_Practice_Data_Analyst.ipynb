{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {float:left}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Practice Data Analyst\n",
    "by NK Zhehua Zou\n",
    "\n",
    "*** Project: Answer the Following Interview Questions (6 total) ***  \n",
    "For each of the questions below, answer as if you were in an interview, explaining and justifying your answer with two to three paragraphs as you see fit. For coding answers, explain the relevant choices you made writing the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Describe a data project you worked on recently.\n",
    "Titanic survival investigation is my favor individual project. I worked with Udacity coaches and students to analyze passengersâ€™ behavior, identify patterns, trends, and variations to help reader to find out what factors will affect the probability of survival. I have developed and implemented data products based on titanic survival dataset. But I found almost half of passengers missing age value, then I evaluated root causes of data issues and suggested corrective action. There have been two options I could take, drop missing values or fill them with the mean value group by sex and pclass. After I tested both of solutions, I shared my result on Udacity forum and discussed with other students which solution is better for this project. I finally chose to drop the missing values as a final decision. I thought it is dishonest to use imaginary values in analyzation and the result looked more accurate and better for data visualization. In the last step, I created data visualizations to track key metrics such as count and correlation of each variable. It helped the readers to understand entire analyzed result within 3 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - You are given a ten piece box of chocolate truffles.\n",
    "You know based on the label that six of the pieces have an orange cream filling and four of the pieces have a coconut filling.  \n",
    "If you were to eat four pieces in a row, what is the probability that the first two pieces you eat have an orange cream filling and the last two have a coconut filling?\n",
    "Follow-up question: If you were given an identical box of chocolates and again eat four pieces in a row, what is the probability that exactly two contain coconut filling?  \n",
    "*** Answers ***  \n",
    "p = target / total  \n",
    "1st pick = 6/10, 2nd pick = 5/9, 3rd pick = 4/8, 4th pick = 3/7  \n",
    "final_p = 6/10 * 5/9 * 4/8 * 3/7 = 0.07143  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Follow-up question: Total probability is 0.428571428571\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# Calculate probability of pulling truffle of given type\n",
    "def pick(quanity, truffle):\n",
    "    total_truffles = sum(quanity.values())\n",
    "    p = (0. + quanity[truffle]) / total_truffles # output float\n",
    "    if quanity[truffle]:\n",
    "        quanity[truffle] -= 1\n",
    "    return p, quanity\n",
    "\n",
    "# define pick_sequences() function to caculate the probability for every eating\n",
    "def pick_sequences(quanity, s):\n",
    "    probs = []\n",
    "    for truffle in s:\n",
    "        p, quanity = pick(quanity, truffle)\n",
    "        probs.append(p)\n",
    "    return reduce(operator.mul, probs, 1)\n",
    "\n",
    "# 6 of sequences can pcik truffles as requirement\n",
    "sequences = ['CCOO', 'OCCO', 'OOCC', 'COOC', 'COCO', 'OCOC']\n",
    "\n",
    "# define pro as the probability for every eating\n",
    "# define total_pro as sum of every probability\n",
    "if __name__ == '__main__':\n",
    "    total_prob = 0\n",
    "    for s in sequences:\n",
    "        quanity = {'O': 6, 'C': 4}\n",
    "        prob = pick_sequences(quanity, s)\n",
    "        \n",
    "        total_prob += prob\n",
    "\n",
    "print 'Follow-up question: Total probability is ' + str(total_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Given the table users:\n",
    "  \n",
    "##### Table \"users\"  \n",
    "| Column | Type |  \n",
    "|---|---|  \n",
    "| id | integer |  \n",
    "| username | character |  \n",
    "| email | character |  \n",
    "| city | character |  \n",
    "| state | character |  \n",
    "| zip | integer |  \n",
    "| active | boolean |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### construct a query to find the top 5 states with the highest number of active users. Include the number for each state in the query result. Example result:  \n",
    "| state | num_active_users |  \n",
    "|---|---|  \n",
    "| New Mexico | 502 |  \n",
    "| Alabama | 495 |  \n",
    "| California | 300 |  \n",
    "| Maine | 201 |  \n",
    "| Texas | 189 |   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This selection implemented by SQL, what is required language in this question\n",
    "We can easier to studying SQL on http://www.w3school.com.cn/sql/\n",
    "'''\n",
    "\n",
    "SELECT state, SUM(active)\n",
    "FROM users\n",
    "GROUP BY state\n",
    "ORDER BY SUM(active) DESC\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "THIS FUNCTION IMPLEMENTED BY PYTHON, I JUST KEEEP IT FOR STUDYING, SKIP IT PLEASE\n",
    "\n",
    "def top_actives(data):\n",
    "    s1 = data.groupby('state').size() # count by each state\n",
    "    s2 = s1.sort_values(axis=0, ascending=False) # sort by count\n",
    "    s3 = pd.Series(s2, name='num_active_users') # add key name for count value \n",
    "    return s3.reset_index() # convert series to dataframe\n",
    "\n",
    "top_actives(users).head(5) \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Define a function first_unique that takes a string as input and returns the first non-repeated (unique) character in the input string. If there are no unique characters return None. Note: Your code should be in Python.\n",
    "  \n",
    "def first_unique(string):  \n",
    "*** Your code here ***  \n",
    " return unique_char\n",
    "\n",
    "> first_unique('aabbcdd123')\n",
    "> c\n",
    "\n",
    "> first_unique('a')\n",
    "> a\n",
    "\n",
    "> first_unique('112233')\n",
    "> None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "a\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# this function only print first unique character\n",
    "\n",
    "def first_unique(string):\n",
    "    Uniq = set(string)\n",
    "    hasUniq = False\n",
    "    for s in Uniq:\n",
    "        if string.count(s)==1: # and s.isdigit()==False JUST ADD THIS IF YOU DON'T WANT TO PRINT INT\n",
    "            return s\n",
    "            hasUniq = True\n",
    "    if hasUniq == False:\n",
    "        return None\n",
    "\n",
    "print first_unique('aabbcdd123')\n",
    "print first_unique('a')\n",
    "print first_unique('112233')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "1\n",
      "3\n",
      "2\n",
      "a\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# this function will print all of unique characters\n",
    "\n",
    "def first_unique(string):\n",
    "    Uniq = set(string)\n",
    "    hasUniq = False\n",
    "    for s in Uniq:\n",
    "        if string.count(s)==1: # and s.isdigit()==False JUST ADD THIS IF YOU DON'T WANT TO PRINT INT\n",
    "            print s\n",
    "            hasUniq = True\n",
    "    if hasUniq == False:\n",
    "        print 'None'\n",
    "\n",
    "first_unique('aabbcdd123')\n",
    "first_unique('a')\n",
    "first_unique('112233')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - What are underfitting and overfitting in the context of Machine Learning? How might you balance them?\n",
    "*** Underfitting *** occurs when a statistical model or machine learning algorithm cannot capture the underlying trend of the data. Intuitively, underfitting occurs when the model or the algorithm does not fit the data well enough. Specifically, underfitting occurs if the model or algorithm shows low variance but high bias. Underfitting is often a result of an excessively simple model.  \n",
    "    \n",
    "*** Overfitting *** refers to a model that models the training data too well. It happens when a model will learn the detail and noise in the training data, this means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model, but the problem is that these concepts do not apply to new data and negatively impact the models ability to generalize.  \n",
    "  \n",
    "*** Comparison ***  \n",
    "Overfitting: too much focus on training set (engineering) and learns complex relations which may not be valid in general for new data (test set)  \n",
    "Underfitting: too little focus on training set. Neither good for training not testing  \n",
    "  \n",
    "*** Ideally, you want to select a model at the sweet spot between underfitting and overfitting. ***\n",
    "Both overfitting and underfitting can lead to poor model performance. But by far the most common problem in applied machine learning is overfitting.  \n",
    "There are two important techniques to limit overfitting:  \n",
    "1 - Use a resampling technique to estimate model accuracy.  \n",
    "2 - Hold back a validation dataset.  \n",
    "k-fold cross validation. It allows you to train and test your model k-times on different subsets of training data and build up an estimate of the performance of a machine learning model on unseen data.  \n",
    "Using cross-validation is a gold standard in applied machine learning for estimating model accuracy on unseen data. If you have the data, using a validation dataset is also an excellent practice.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - If you were to start your data analyst position today, what would be your goals a year from now?\n",
    "Thank you for giving me this opportunity to join your company. My goal is become a senior Data Analyst within this year. Within first 3 months, I need to understand every single table in the database in order to do fundamental data analysis such as data visualization, statistical inference, etc. I also need to improve my communication skills in order to solve the problem without a supervisor. After first three months, I need to make sure I will familiar with bash scripting, and NoSQL database, in order to code fluently in SQL and Python to develop new innovative metrics to meet requirements as senior Data Analyst. I also want to improve my skills to help me create new products for company. I will query the database and create a hypothesis and test it, then target those new emerging segment if this hypothesis is true. In the meantime, I would like to use machine learning to analyze and predict data and make a recommendation to customers to help them improve efficiency."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
